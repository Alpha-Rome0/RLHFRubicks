{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T16:32:56.157463Z",
     "start_time": "2024-04-17T16:32:56.066103Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch, os\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, GemmaTokenizer, TrainingArguments\n",
    "from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer\n",
    "from torch.utils.data import DataLoader\n",
    "from reward import reward_model_strict\n",
    "from peft import LoraConfig\n",
    "import bitsandbytes as bnb\n",
    "import csv, json\n",
    "from dotenv import load_dotenv\n",
    "from datasets import Dataset, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T16:32:56.157463Z",
     "start_time": "2024-04-17T16:32:56.066103Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-17T16:32:56.157463Z",
     "start_time": "2024-04-17T16:32:56.066103Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "LR = 1.41e-5\n",
    "BATCH_SIZE = 1\n",
    "MINI_BATCH_SIZE = 1\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    #bnb_4bit_quant_type=\"nf4\",\n",
    "    #bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "model_id = \"../gemma-2b-sft_old\"\n",
    "#model_id = \"vicgalle/gpt2-open-instruct-v1\"\n",
    "config = PPOConfig(\n",
    "    model_name=model_id,\n",
    "    learning_rate=LR,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    mini_batch_size=MINI_BATCH_SIZE\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:`peft_config` argument ignored since a peft config file was found in ../gemma-2b-sft_old\n",
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fedebe9f18a4b5896438163eeb27834",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:A <class 'peft.peft_model.PeftModelForCausalLM'> model is loaded from '../gemma-2b-sft_old', and no v_head weight is found. This IS expected if you are not resuming PPO training.\n",
      "WARNING:root:A <class 'peft.peft_model.PeftModelForCausalLM'> model is loaded from '../gemma-2b-sft_old', and no v_head weight is found. This IS expected if you are not resuming PPO training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name, token=os.environ['HF_TOKEN'])\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name, quantization_config=bnb_config, peft_config=lora_config, device_map=\"auto\", token=os.environ['HF_TOKEN'])\n",
    "#model = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name,  device_map=\"auto\", token=os.environ['HF_TOKEN'])\n",
    "\n",
    "optimizer = bnb.optim.Adam8bit(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_jsonl(csv_path, jsonl_path):\n",
    "    with open('../ttt_prompt.txt', 'r') as file:\n",
    "        # Read the entire file into a string\n",
    "        prompt = file.read()\n",
    "    with open(csv_path, 'r') as csv_file, open(jsonl_path, 'w') as jsonl_file:\n",
    "        reader = csv.DictReader(csv_file)\n",
    "        for row in reader:\n",
    "            jsonl_file.write(\n",
    "                json.dumps({\"prompt\": prompt.format(state = row[\"Game States\"]), \"completion\": f'{row[\"Optimal Moves\"]}'}) + \"\\n\")\n",
    "            \n",
    "def tokenize(sample):\n",
    "    sample[\"input_ids\"] = tokenizer.encode(sample[\"prompt\"])\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49e592204d5f4beda962777831ab0570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c2d618465fb4cc185a8fd19bb238acf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "csv_to_jsonl('../examples/ttt_data_ppo_train.csv', \"data.jsonl\")\n",
    "dataset = load_dataset(\"json\", data_files=\"data.jsonl\", split='train')\n",
    "dataset = dataset.map(tokenize, batched=False)\n",
    "dataset = dataset.with_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'You are a tic-tac-toe solver. A tic-tac-toe board is a 3x3 grid. For example\\n\\nb,o,b\\nx,b,b\\nb,b,o\\n\\nb represents an empty position\\no represents a mark by player 1\\nx represents a mark by player 2\\n\\nThis state can also be represented in one line eg.\\nbobxbbbbo\\n\\nThe grid is also numbered where each number represents a position on the grid. eg.\\n1,2,3\\n4,5,6\\n7,8,9\\n\\na move can thus be represented by mark+number. Here are some examples:\\no5 means player 1 marks position 5 on the grid\\nx1 means player 2 marks positoin 4 on the grid\\n\\nYour job is to generate the next best move given a tic-tac-toe board state.\\n\\nYou must only answer with mark+number format and nothing else eg:\\no7\\n\\n\\nGiven the following state, what is the next best move?\\nxobxoboxb\\n\\nThe next best move is ', 'completion': 'o3', 'input_ids': tensor([     2,   2045,    708,    476,  62859, 235290,  33638, 235290,  59771,\n",
      "         75921, 235265,    586,  62859, 235290,  33638, 235290,  59771,   4924,\n",
      "           603,    476, 235248, 235304, 235297, 235304,  11297, 235265,   1699,\n",
      "          3287,    109, 235268, 235269, 235253, 235269, 235268,    108, 235297,\n",
      "        235269, 235268, 235269, 235268,    108, 235268, 235269, 235268, 235269,\n",
      "        235253,    109, 235268,  12786,    671,   8144,   3668,    108, 235253,\n",
      "         12786,    476,   2110,    731,   5398, 235248, 235274,    108, 235297,\n",
      "         12786,    476,   2110,    731,   5398, 235248, 235284,    109,   1596,\n",
      "          2329,    798,   1170,    614,  12754,    575,    974,   2017,   7815,\n",
      "        235265,    108,  26242, 235297,  93398,   1055,    109,    651,  11297,\n",
      "           603,   1170,  45760,   1570,   1853,   1758,  12786,    476,   3668,\n",
      "           611,    573,  11297, 235265,   7815, 235265,    108, 235274, 235269,\n",
      "        235284, 235269, 235304,    108, 235310, 235269, 235308, 235269, 235318,\n",
      "           108, 235324, 235269, 235321, 235269, 235315,    109, 235250,   3124,\n",
      "           798,   6319,    614,  12754,    731,   2110, 235340,   4308, 235265,\n",
      "          5698,    708,   1009,   8944, 235292,    108, 235253, 235308,   3454,\n",
      "          5398, 235248, 235274,  12659,   3668, 235248, 235308,    611,    573,\n",
      "         11297,    108, 235297, 235274,   3454,   5398, 235248, 235284,  12659,\n",
      "          1720,   2355,    473, 235248, 235310,    611,    573,  11297,    109,\n",
      "          6922,   3356,    603,    577,  11941,    573,   2351,   1963,   3124,\n",
      "          2764,    476,  62859, 235290,  33638, 235290,  59771,   4924,   2329,\n",
      "        235265,    109,   2045,   2004,   1297,   3448,    675,   2110, 235340,\n",
      "          4308,   5920,    578,   4285,   1354,   7815, 235292,    108, 235253,\n",
      "        235324,    110,  24985,    573,   2412,   2329, 235269,   1212,    603,\n",
      "           573,   2351,   1963,   3124, 235336,    108, 235297,   1023, 235297,\n",
      "         70411, 235268,    109,    651,   2351,   1963,   3124,    603, 235248])}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.utils.other:Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "ppo_trainer = PPOTrainer(\n",
    "    model=model,\n",
    "    config=config,\n",
    "    tokenizer=tokenizer,\n",
    "    optimizer= optimizer,\n",
    "    dataset=dataset\n",
    ")\n",
    "\n",
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"max_new_tokens\": 4,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/heyandy/.conda/envs/RLProject/lib/python3.11/site-packages/transformers/generation/utils.py:1542: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n",
      "/home/heyandy/.conda/envs/RLProject/lib/python3.11/site-packages/bitsandbytes/nn/modules.py:426: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a tic-tac-toe solver. A tic-tac-toe board is a 3x3 grid. For example\n",
      "\n",
      "b,o,b\n",
      "x,b,b\n",
      "b,b,o\n",
      "\n",
      "b represents an empty position\n",
      "o represents a mark by player 1\n",
      "x represents a mark by player 2\n",
      "\n",
      "This state can also be represented in one line eg.\n",
      "bobxbbbbo\n",
      "\n",
      "The grid is also numbered where each number represents a position on the grid. eg.\n",
      "1,2,3\n",
      "4,5,6\n",
      "7,8,9\n",
      "\n",
      "a move can thus be represented by mark+number. Here are some examples:\n",
      "o5 means player 1 marks position 5 on the grid\n",
      "x1 means player 2 marks positoin 4 on the grid\n",
      "\n",
      "Your job is to generate the next best move given a tic-tac-toe board state.\n",
      "\n",
      "You must only answer with mark+number format and nothing else eg:\n",
      "o7\n",
      "\n",
      "\n",
      "Given the following state, what is the next best move?\n",
      "xobxoboxb\n",
      "\n",
      "The next best move is \n",
      "<strong>o1\n"
     ]
    }
   ],
   "source": [
    "response_tensors = ppo_trainer.generate(dataset[0]['input_ids'], **generation_kwargs)\n",
    "print(tokenizer.decode(response_tensors[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]/home/heyandy/.conda/envs/RLProject/lib/python3.11/site-packages/transformers/generation/utils.py:1542: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n",
      "/home/heyandy/.conda/envs/RLProject/lib/python3.11/site-packages/trl/trainer/ppo_trainer.py:1285: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ../aten/src/ATen/native/ReduceOps.cpp:1807.)\n",
      "  std_scores = data[\"scores\"].std()\n",
      "/home/heyandy/.conda/envs/RLProject/lib/python3.11/site-packages/trl/trainer/ppo_trainer.py:1312: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ../aten/src/ATen/native/ReduceOps.cpp:1807.)\n",
      "  stats[\"tokens/queries_len_std\"] = torch.std(query_lens).cpu().numpy().item()\n",
      "/home/heyandy/.conda/envs/RLProject/lib/python3.11/site-packages/trl/trainer/ppo_trainer.py:1315: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ../aten/src/ATen/native/ReduceOps.cpp:1807.)\n",
      "  stats[\"tokens/responses_len_std\"] = torch.std(response_lens).cpu().numpy().item()\n",
      "  0%|          | 5/10000 [00:10<5:32:46,  2.00s/it]"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for sample in tqdm(dataset):\n",
    "        query = sample['input_ids']\n",
    "        response_tensor = ppo_trainer.generate(query, return_prompt = False, **generation_kwargs)\n",
    "        response = tokenizer.decode(response_tensor[0], skip_special_tokens=True)\n",
    "        correct_answer = sample['completion']\n",
    "        #### Compute reward score\n",
    "        reward = 0\n",
    "        if correct_answer in response:\n",
    "            reward = 1.0\n",
    "        #print(response, sample['completion'], reward)\n",
    "        #### Run PPO step\n",
    "        stats = ppo_trainer.step([query], [response_tensor[0]], [torch.tensor(reward, dtype=torch.float)])\n",
    "\n",
    "        #TODO: log stats\n",
    "        # ppo_trainer.log_stats(stats, batch, rewards)\n",
    "\n",
    "#### Save model\n",
    "ppo_trainer.save_model(f\"gemma-2b-rlhf-ttt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RLproject",
   "language": "python",
   "name": "rlproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
