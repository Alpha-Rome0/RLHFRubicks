{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbd3b53c28fb7674",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T15:22:51.654787Z",
     "start_time": "2024-04-26T15:22:38.171628Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import torch, csv, json\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "import bitsandbytes as bnb\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
    "from peft import LoraConfig\n",
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14a2092c523f52c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T15:22:51.666510Z",
     "start_time": "2024-04-26T15:22:51.656223Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T15:22:51.674556Z",
     "start_time": "2024-04-26T15:22:51.668077Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def csv_to_jsonl(csv_path, jsonl_path):\n",
    "    with open('ttt_prompt.txt', 'r') as file:\n",
    "        # Read the entire file into a string\n",
    "        prompt = file.read()\n",
    "    with open(csv_path, 'r') as csv_file, open(jsonl_path, 'w') as jsonl_file:\n",
    "        reader = csv.DictReader(csv_file)\n",
    "        for row in reader:\n",
    "            jsonl_file.write(\n",
    "                json.dumps({\"prompt\": prompt.format(state = row[\"Game States\"]), \"completion\": f'{row[\"Optimal Moves\"]}'}) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1edd1a03ef45bb8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T15:22:51.836459Z",
     "start_time": "2024-04-26T15:22:51.677007Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Load the csv file into a pandas DataFrame\n",
    "df = pd.read_csv('../examples/ttt_data.csv')\n",
    "csv_to_jsonl('../examples/ttt_data.csv', \"data.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "441317d100577027",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T15:22:52.542607Z",
     "start_time": "2024-04-26T15:22:51.837360Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b41a18b0c5344ab5ba7c064ee2456b12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"json\", data_files=\"data.jsonl\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5c9ca9acecb266a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T15:22:52.552318Z",
     "start_time": "2024-04-26T15:22:52.543786Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are a tic-tac-toe solver. A tic-tac-toe board is a 3x3 grid. For example\\n\\nb,o,b\\nx,b,b\\nb,b,o\\n\\nb represents an empty position\\no represents a mark by player 1\\nx represents a mark by player 2\\n\\nThis state can also be represented in one line eg.\\nbobxbbbbo\\n\\nThe grid is also numbered where each number represents a position on the grid. eg.\\n1,2,3\\n4,5,6\\n7,8,9\\n\\na move can thus be represented by mark+number. Here are some examples:\\no5 means player 1 marks position 5 on the grid\\nx1 means player 2 marks positoin 4 on the grid\\n\\nYour job is to generate the next best move given a tic-tac-toe board state.\\n\\nYou must only answer with mark+number format and nothing else eg:\\no7\\n\\n\\nGiven the following state, what is the next best move?\\nxbboxbbbo\\n\\nThe next best move is '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[42]['prompt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd6ce8380afbc570",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T15:22:52.565593Z",
     "start_time": "2024-04-26T15:22:52.553076Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'o3'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[42]['completion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6d4ab3efdae3169",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T15:22:52.585399Z",
     "start_time": "2024-04-26T15:22:52.566563Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model_id = \"google/gemma-2b\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86d04c11d1ba8993",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T18:44:53.367543Z",
     "start_time": "2024-04-25T18:44:46.837229Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58a217b6ae2041d5b06e0f47cfffa9ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map='auto')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86317d2f451a44e6",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-04-25T18:46:42.888716Z"
    },
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/heyandy/.conda/envs/RLProject/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:246: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c8fd021c4a8441d9a47de7da2e2963c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9999 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No chat template is set for this tokenizer, falling back to a ChatML template. This is very error-prone, because most models are not trained with a ChatML template!Default chat templates are a legacy feature and will be removed in Transformers v4.43, at which point any code depending on them will stop working. We recommend setting a valid chat template before then to ensure that this model continues working without issues.\n",
      "/home/heyandy/.conda/envs/RLProject/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "WARNING:accelerate.utils.other:Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7497' max='7497' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7497/7497 1:36:22, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.088200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.036800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.034700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.033900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.032200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.030200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.030400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.030100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.029400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.028700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.027800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.027700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.027400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.027100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=2,\n",
    "        #max_steps=30,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=500,\n",
    "        output_dir=\"outputs\",\n",
    "        optim=\"paged_adamw_8bit\"\n",
    "    ),\n",
    "    peft_config=lora_config,\n",
    ")\n",
    "trainer.train()\n",
    "trainer.save_model(\"../gemma-2b-sft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c3e3bfb-146d-4be2-9ae0-4279cae01b42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T14:58:44.710506Z",
     "start_time": "2024-04-26T14:58:38.412650Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48c711b956204ad288be3376be8cd5a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"../gemma-2b-sft\", quantization_config=bnb_config, device_map='auto')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../gemma-2b-sft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9427750-64c1-43b3-ae82-285745489580",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-26T14:58:44.710506Z",
     "start_time": "2024-04-26T14:58:38.412650Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<strong>x5</strong>.\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(dataset[42]['prompt'], return_tensors=\"pt\", return_attention_mask=False)\n",
    "outputs = model.generate(**inputs, max_new_tokens=5)\n",
    "\n",
    "text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "prompt_len =  len(dataset[42]['prompt'])\n",
    "print(text[prompt_len:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6a5aa263ef379cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-25T18:45:22.124711Z",
     "start_time": "2024-04-25T18:45:22.122664Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8b82588f3514933bacec5da13a9a217",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "csv_to_jsonl('../examples/ttt_data_test.csv', \"data.jsonl\")\n",
    "dataset = load_dataset(\"json\", data_files=\"data.jsonl\", split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b175d5e1-ccb3-437e-92c8-aad1e48031fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:50<00:00,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "n = 100\n",
    "for sample in tqdm(dataset.select(range(n))):\n",
    "    inputs = tokenizer(sample['prompt'], return_tensors=\"pt\", return_attention_mask=False)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=4)\n",
    "    text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "    prompt_len = len(sample['prompt'])\n",
    "    #print(text[prompt_len:], sample['completion'])\n",
    "    if sample['completion'] in text[prompt_len:]:\n",
    "        correct += 1\n",
    "print(correct/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31eb9cf-863a-40a4-bfc3-c632c43a5ce5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RLproject",
   "language": "python",
   "name": "rlproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
